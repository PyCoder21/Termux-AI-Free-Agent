"""
ai.py
This is the project's main file
"""

import sys
import argparse
import json
import os
from typing import List, Dict, Any
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import HumanMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from prompt_toolkit import PromptSession
from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
from prompt_toolkit.history import FileHistory
from prompt_toolkit.lexers import PygmentsLexer
from prompt_toolkit.styles import Style
from pygments.lexers.shell import BashLexer
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.markup import escape
from rich.text import Text
from rich.console import Group
from tools import get_tools

# ==============================================================================
# 1. –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
# ==============================================================================


console = Console(log_path=False)


def load_config() -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Ñ–∞–π–ª–∞ config.json."""
    try:
        with open("/data/data/com.termux/files/home/Termux-AI-Free-Agent/config.json", "r") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        console.print(f"[bold red]–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ config.json:[/]{e}")
        console.print("[yellow]–°–æ–∑–¥–∞–π—Ç–µ config.json —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –∫–ª—é—á–∞–º–∏ (model, base_url).[/]")
        sys.exit(1)


CONFIG = load_config()

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏ –≤ —Ç–æ–∫–µ–Ω–∞—Ö.
# –ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 128000 –¥–ª—è gpt-4-turbo)
MAX_CONTEXT_TOKENS = 128000

# ==============================================================================
# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è API –∏ –æ–±–µ—Ä—Ç–æ–∫
# ==============================================================================


# ==============================================================================
# 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–≤–æ–¥–∞ –∏ –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
# ==============================================================================

class StreamingOutputHandler(BaseCallbackHandler):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ –æ—Ç LLM, —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É—è –µ–≥–æ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏."""
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        console.print(token, end="", style="bold cyan")

def display_tool_call(tool_call: Dict[str, Any]):
    """–ö—Ä–∞—Å–∏–≤–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≤—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞."""
    tool_name = tool_call['name']
    tool_args = tool_call['args']
    
    # –°–æ–∑–¥–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    header = Text.from_markup(f"[bold]–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç:[/] [cyan]{tool_name}[/]\n[bold]–ê—Ä–≥—É–º–µ–Ω—Ç—ã:[/]\n")
    args_str = json.dumps(tool_args, indent=2, ensure_ascii=False)
    syntax = Syntax(args_str, "json", theme="monokai", line_numbers=True)

    # –°–æ–∑–¥–∞–µ–º –≥—Ä—É–ø–ø—É —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    content = Group(header, syntax)

    # –ü–µ—á–∞—Ç–∞–µ–º –ø–∞–Ω–µ–ª—å
    console.print(Panel(content, title="[yellow]–í—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞", border_style="yellow"))

def process_tool_calls(tool_calls: List[Dict[str, Any]], tools: List) -> List[ToolMessage]:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤—ã–∑–æ–≤—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
    tool_messages = []
    tool_map = {t.name: t for t in tools}

    for tool_call in tool_calls:
        display_tool_call(tool_call)
        if func := tool_map.get(tool_call['name']):
            try:
                result = func.invoke(tool_call['args'])
                console.print(Panel(
                    f"[bold green]'{tool_call['name']}' : [/]{escape(str(result))}",
                    border_style="green",
                    title="[green]–†–µ–∑—É–ª—å—Ç–∞—Ç"
                ))
                tool_messages.append(ToolMessage(
                    content=str(result),
                    name=tool_call['name'],
                    tool_call_id=tool_call['id']
                ))
            except Exception as e:
                error_message = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ '{tool_call['name']}': {escape(str(e))}"
                console.print(Panel(error_message, title="[bold red]–û—à–∏–±–∫–∞", border_style="red"))
                tool_messages.append(ToolMessage(
                    content=error_message,
                    name=tool_call['name'],
                    tool_call_id=tool_call['id']
                ))
        else:
            error_message = f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {tool_call['name']}"
            console.print(f"[bold red]{error_message}[/]")
            tool_messages.append(ToolMessage(
                content=error_message,
                name=tool_call['name'],
                tool_call_id=tool_call['id']
            ))
    return tool_messages

# ==============================================================================
# 5. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (CLI)
# ==============================================================================

def create_llm_chain(
    config: Dict[str, Any],
    tools: List,
    is_interactive_mode: bool,
    *,
    use_gpt: bool = False,
    use_qwen: bool = False,
    use_gemini: bool = False,
    use_deepseek: bool = False,
    use_kimi: bool = False
) -> Any:
    use_together = False

    global mo
    """–°–æ–∑–¥–∞–µ—Ç —Ü–µ–ø–æ—á–∫—É LLM —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏."""
    if use_qwen:
        mo = "Qwen/Qwen3-235B-A22B-fp8-tput"
    elif use_gpt:
        mo = "gpt-5"
    elif use_gemini:
        mo = "google/gemini-2.5-pro-preview-05-06"
    elif use_deepseek:
        mo = "deepseek-ai/DeepSeek-V3"
        use_together = True
    elif use_kimi:
        mo = "moonshotai/Kimi-K2-Instruct"
        use_together = True
    else:
        # –ú–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        if config.get("default_model") == "qwen":
            mo = "Qwen/Qwen3-235B-A22B-fp8-tput"
        elif config.get("default_model") == "gpt":
            mo = "gpt-5"
        elif config.get("default_model") == "gemini-2.5-pro":
            mo = "google/gemini-2.5-pro-preview-05-06"
        elif config.get("default_model") == "deepseek-v3":
            mo = "deepseek-ai/DeepSeek-V3"
            use_together = True
        elif config.get("default_model") == "kimi-k2":
            mo = "moonshotai/Kimi-K2-Instruct"
            use_together = True
        else:
            print("set default model to qwen, gpt, gemini-2.5-pro, deepseek-v3 or kimi-k2.")
            sys.exit(1)
    if use_together:
        llm = ChatOpenAI(
            api_key="56c8eeff9971269d7a7e625ff88e8a83a34a556003a5c87c289ebe9a3d8a3d2c",
            model=mo,
            streaming=True,
            base_url="https://api.together.xyz/v1",
            temperature=0.1,
        )
    else:
        llm = ChatOpenAI(
            api_key = "sk-",
            model=mo,
            streaming=True,
            base_url="http://127.0.0.1:61252/v1",
            temperature=0.1,
        )
    # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    system_prompt = f"""
–¢—ã ‚Äî AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ —Å—Ä–µ–¥–µ Termux. –¢—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –ò–ò –º–æ–¥–µ–ª—å {mo}. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–º–æ–≥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –≤—ã–ø–æ–ª–Ω—è—è –∑–∞–¥–∞—á–∏ —à–∞–≥ –∑–∞ —à–∞–≥–æ–º.
- **–û–¥–∏–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∑–∞ —Ä–∞–∑:** –í –∫–∞–∂–¥–æ–º –æ—Ç–≤–µ—Ç–µ –≤—ã–∑—ã–≤–∞–π –Ω–µ –±–æ–ª–µ–µ –û–î–ù–û–ì–û –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.
- **–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** –†–∞–±–æ—Ç–∞–π –ø–æ —Ü–∏–∫–ª—É: "–æ—Ç–≤–µ—Ç -> –≤—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ -> –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç -> –≤—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞...", –ø–æ–∫–∞ –∑–∞–¥–∞—á–∞ –Ω–µ –±—É–¥–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ—à–µ–Ω–∞.
- **–¢–æ—á–Ω–æ—Å—Ç—å:** –ë—É–¥—å –ø—Ä–µ–¥–µ–ª—å–Ω–æ —Ç–æ—á–Ω—ã–º –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ñ–∞–π–ª–∞–º–∏ –∏ –∫–æ–º–∞–Ω–¥–∞–º–∏.
- **–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã:** –î–ª—è –ø–æ–≥–æ–¥—ã –∏—Å–ø–æ–ª—å–∑—É–π —à–∏—Ä–æ—Ç—É –∏ –¥–æ–ª–≥–æ—Ç—É, –æ–∫—Ä—É–≥–ª–µ–Ω–Ω—ã–µ –¥–æ –¥–≤—É—Ö –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 55.75, 37.62 –¥–ª—è –ú–æ—Å–∫–≤—ã).
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç Termux:** –ü–æ–º–Ω–∏, —á—Ç–æ —Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –≤ Termux. –ê–¥–∞–ø—Ç–∏—Ä—É–π –∫–æ–º–∞–Ω–¥—ã –∏ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –ø–æ–¥ —ç—Ç—É —Å—Ä–µ–¥—É. –ü—Ä–∏ –ø–æ–∏—Å–∫–µ –æ—à–∏–±–æ–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, —Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –æ–±—â–µ–π —á–∞—Å—Ç–∏ –æ—à–∏–±–∫–∏, –∞ –Ω–µ –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è Termux –ø—É—Ç—è—Ö.
- **–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π:** –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å, –∫–∞–∫ —á—Ç–æ-—Ç–æ —Å–¥–µ–ª–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–∏—Å–∫–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã.
- run_cmd_pexpect –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –∫–æ–º–∞–Ω–¥—ã –ü–û–õ–ù–û–°–¢–¨–Æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ, –¥–∞–∂–µ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ, –Ω–æ –Ω–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã —Å curses, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –ª–æ–º–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω–∞–ª.

–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π MarkDown, –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ—Ç–≤–µ—á–∞–π –ø–æ–Ω—è—Ç–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º.
"""
    if not is_interactive_mode:
        system_prompt += """

–í–ù–ò–ú–ê–ù–ò–ï: –¢—ã –Ω–∞—Ö–æ–¥–∏—à—å—Å—è –≤ –ù–ï–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ú —Ä–µ–∂–∏–º–µ. –¢—ã –î–û–õ–ñ–ï–ù –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É –ø–æ–ª–Ω–æ—Å—Ç—å—é, –Ω–µ –æ–∂–∏–¥–∞—è —É—Ç–æ—á–Ω–µ–Ω–∏–π –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ï—Å–ª–∏ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å, –∫–∞–∫ –ø–æ—Å—Ç—É–ø–∏—Ç—å, –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –∏ –ø—Ä–æ–¥–æ–ª–∂–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ. –ù–ï –ó–ê–î–ê–í–ê–ô –í–û–ü–†–û–°–û–í.
"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages")
    ])
    llm_with_tools = llm.bind_tools(tools)
    return prompt | llm_with_tools

def compress_chat_history(chat_history: List, config: Dict[str, Any]) -> List:
    """–°–∂–∏–º–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—É—é –∏—Å—Ç–æ—Ä–∏—é."""
    console.print("[bold yellow]–°–∂–∞—Ç–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞...[/]")

    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –¥–ª—è —Å–∂–∞—Ç–∏—è
    compressor_llm = ChatOpenAI(
        api_key="sk-",
        model="gpt-5",
        streaming=True,
        base_url="http://127.0.0.1:61252/v1",
        temperature=0.1,
    )

    # –ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–∂–∞—Ç–∏—è
    compression_prompt_text = 'Summarize our conversation up to this point. The summary should be a concise yet comprehensive overview of all key topics, questions, answers, and important details discussed. This summary will replace the current chat history to conserve tokens, so it must capture everything essential to understand the context and continue our conversation effectively as if no information was lost.'
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–∂–∞—Ç–∏—è
    compression_prompt = ChatPromptTemplate.from_messages([
        MessagesPlaceholder(variable_name="messages"),
        ("user", compression_prompt_text)
    ])

    chain = compression_prompt | compressor_llm

    try:
        response = chain.invoke({"messages": chat_history})
        summary = response.content
        console.print(Panel(f"[bold green]–ò—Å—Ç–æ—Ä–∏—è —É—Å–ø–µ—à–Ω–æ —Å–∂–∞—Ç–∞.[/]\n[dim]{summary}[/dim]", border_style="green"))
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ–≤—É—é –∏—Å—Ç–æ—Ä–∏—é, —Å–æ—Å—Ç–æ—è—â—É—é –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è-—Å–∞–º–º–∞—Ä–∏
        return [HumanMessage(content=f"This is a summary of the previous conversation:\n{summary}")]

    except Exception as e:
        console.print(f"[bold red]–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∂–∞—Ç–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏: {escape(str(e))}[/]")
        return chat_history # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞—Ä—É—é –∏—Å—Ç–æ—Ä–∏—é –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏



def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∑–∞–ø—É—Å–∫–∞—é—â–∞—è CLI."""
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = argparse.ArgumentParser()
    parser.add_argument('query', nargs='*', help='–ó–∞–ø—Ä–æ—Å –¥–ª—è –Ω–µ–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞')
    parser.add_argument('--gpt', action='store_true', help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OpenAI GPT')
    parser.add_argument('--qwen', action='store_true', help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Qwen')
    parser.add_argument('--gemini', action='store_true', help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Gemini')
    parser.add_argument('--deepseek', action='store_true', help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DeepSeek-V3')
    parser.add_argument('--kimi', action='store_true', help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Kimi-K2 (1 —Ç—Ä–∏–ª–ª–∏–æ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤!)')
    args = parser.parse_args()

    is_interactive_mode = not args.query
    initial_query = " ".join(args.query) if args.query else None
    console.print(Panel.fit(
        "[bold magenta]ü§ñ AI –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è Termux[/]",
        subtitle="[cyan]üì± + üê≥ + ü¶ú = üî•[/]",
        border_style="blue"
    ))

    if is_interactive_mode:
        console.print("[dim]–í–≤–µ–¥–∏—Ç–µ 'exit' –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+D –¥–ª—è –≤—ã—Ö–æ–¥–∞.[/]")
        session = PromptSession(
            history=FileHistory('.assistant_history'),
            auto_suggest=AutoSuggestFromHistory(),
            lexer=PygmentsLexer(BashLexer),
            style=Style.from_dict({'prompt': 'bold ansigreen', 'input': 'bold'})
        )
    else:
        console.print("[bold yellow]–ó–∞–ø—É—â–µ–Ω –Ω–µ–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º.[/]")
        console.print("[dim]–ó–∞–¥–∞—á–∞ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –±–µ–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.[/]")
        session = None

    tools = get_tools()
    chain = create_llm_chain(
        CONFIG,
        tools,
        is_interactive_mode,
        use_gpt=args.gpt,
        use_qwen=args.qwen,
        use_gemini=args.gemini,
        use_deepseek=args.deepseek,
        use_kimi=args.kimi
    )
    chat_history = []
    last_prompt_tokens = 0
    console.log("Model:", mo)
    while True:
        try:
            if is_interactive_mode:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –≤–≤–æ–¥–æ–º
                if last_prompt_tokens > 0:
                    context_percent = min(100.0, (last_prompt_tokens / MAX_CONTEXT_TOKENS) * 100)
                    bar_length = 20
                    filled = int(bar_length * context_percent / 100)
                    indicator = '‚ñà' * filled + '‚ñë' * (bar_length - filled)
                    console.print(f"[dim]–ö–æ–Ω—Ç–µ–∫—Å—Ç: [{('green' if context_percent < 70 else 'yellow' if context_percent < 90 else 'red')}]{indicator}[/] [green]{context_percent:.1f}%[/] ({last_prompt_tokens}/{MAX_CONTEXT_TOKENS} —Ç–æ–∫–µ–Ω–æ–≤)[/]")

                user_input = session.prompt([('class:prompt', '[–í–∞—à –∑–∞–ø—Ä–æ—Å] ‚û§ ')])
                if user_input.lower().strip() in ('exit', 'quit', 'q'):
                    break
                if user_input.lower().strip() == '/compress':
                    if len(chat_history) > 1:
                        chat_history = compress_chat_history(chat_history, CONFIG)
                        last_prompt_tokens = 0 # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã, —á—Ç–æ–±—ã –æ–Ω–∏ –ø–µ—Ä–µ—Å—á–∏—Ç–∞–ª–∏—Å—å –Ω–∞ —Å–ª–µ–¥. —à–∞–≥–µ
                    else:
                        console.print("[yellow]–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞ –¥–ª—è —Å–∂–∞—Ç–∏—è.[/]")
                    continue

                if not user_input.strip():
                    continue
            else:
                user_input = initial_query
                if not user_input:
                    console.print("[bold red]–û—à–∏–±–∫–∞: –í –Ω–µ–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å.[/]")
                    break
                console.print(f"[bold green]–ó–∞–ø—Ä–æ—Å:[/][cyan] {user_input}[/]")
                initial_query = None 
            chat_history.append(HumanMessage(content=user_input))
            max_iterations = 50
            for i in range(max_iterations):
                try:
                    response = chain.invoke(
                        {"messages": chat_history},
                        config=RunnableConfig(callbacks=[StreamingOutputHandler()])
                    )
                except Exception as e:
                    console.print("[bold red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–æ–¥–µ–ª–∏:[/]")
                    console.print(escape(str(e)))
                    break
                
                console.print()

                # –ü–æ–∫–∞–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–∞—Ö
                if hasattr(response, "usage_metadata") and response.usage_metadata:
                    usage = response.usage_metadata
                    prompt = usage.get('prompt_tokens') or usage.get('input_tokens', 0)
                    completion = usage.get('completion_tokens') or usage.get('output_tokens', 0) or usage.get('generated_tokens', 0)
                    total = usage.get('total_tokens', 0)

                    # –ï—Å–ª–∏ total –≤—Å–µ –µ—â–µ 0, –Ω–æ –µ—Å—Ç—å prompt –∏ completion, –ø–æ—Å—á–∏—Ç–∞–µ–º –µ–≥–æ
                    if total == 0 and (prompt > 0 or completion > 0):
                        total = prompt + completion
                    
                    last_prompt_tokens = prompt # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏

                    console.print(f"[dim]–¢–æ–∫–µ–Ω—ã: [green]prompt={prompt} completion={completion} total={total}[/]")
                else:
                    last_prompt_tokens = 0 # –°–±—Ä–∞—Å—ã–≤–∞–µ–º, –µ—Å–ª–∏ –∏–Ω—Ñ–æ –Ω–µ—Ç
                    console.print("[yellow]–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∞—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞[/]")
                
                if response.tool_calls:
                    tool_messages = process_tool_calls(response.tool_calls, tools)
                    chat_history.append(response)
                    chat_history.extend(tool_messages)
                else:
                    chat_history.append(response)
                    console.print(Panel("[bold green]‚úì –ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞[/]", border_style="green"))
                    break
            else:
                console.print(Panel("[bold yellow]‚ö† –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π. –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ –Ω–µ —Ä–µ—à–µ–Ω–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å.[/]", border_style="yellow"))

            if not is_interactive_mode:
                break

        except (KeyboardInterrupt, EOFError):
            break
        except Exception as e:
            console.print("[bold red]–ü—Ä–æ–∏–∑–æ—à–ª–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞:[/]")
            console.print(escape(str(e)))
            
    console.print("[bold green]–í—ã—Ö–æ–¥...[/]")

if __name__ == "__main__":
    main()
